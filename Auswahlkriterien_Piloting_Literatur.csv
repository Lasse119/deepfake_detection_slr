ID;Title;Authors;Year;Abstract;Keywords;Database;Manual Decision
2;Deepfake Creation and Detection using Ensemble Deep Learning Models;Rao, Sanjeev and Shelke, Nitin Arvind and Goel, Aditya and Bansal, Harshita;2022;The use of Artificial Intelligence to create falsified videos using Deep Neural Networks is posing a serious problem in distinguishing the real from the counterfeit. These counterfeit videos are known as Deepfakes. Due to their realistic appearance and their subsequent ability to influence perceptions and mass sentiment, deepfakes must be monitored. Malicious deepfakes must be detected, and their circulation is immediately controlled. Many deepfake detection technologies have been developed that use particular features to classify fabricated media. This paper proposes the framework of deepfake detection using deep neural network models. The hybrid combination of deep learning models predicts deepfakes with better accuracy. The proposed model is tested and evaluated on the DFDC and CelebDF dataset that classifies more deepfake videos.;LSTM, GANs, Deepfakes, Deep Learning, Artificial Intelligence;ACM;
6;Security Implications of Deepfakes in Face Authentication;Salko, Milan and Firc, Anton and Malinka, Kamil;2024;Deepfakes are media generated by deep learning and are nearly indistinguishable from real content to humans. Deepfakes have seen a significant surge in popularity in recent years. There have been numerous papers discussing their effectiveness in deceiving people. What's equally, if not more concerning, is the potential vulnerability of facial and voice recognition systems to deepfakes. The misuse of deepfakes to spoof automated facial recognition systems can threaten various aspects of our lives, including financial security and access to secure locations. This issue remains largely unexplored. Thus, this paper investigates the technical feasibility of a spoofing attack on facial recognition. Firstly, we perform a threat analysis to understand what facial recognition use cases allow the execution of deepfake spoofing attacks. Based on this analysis, we define the attacker model for these attacks on facial recognition systems. Then, we demonstrate the ability of deepfakes to spoof two commercial facial recognition systems. Finally, we discuss possible means to prevent such spoofing attacks.;deepfake, facial recognition, biometrics systems, machine learning, computer security;ACM;FALSCH
9;IoT based application designing of Deep Fake Test for Face animation;Sridevi, Kotari and Kanaprthi. Suresh Kumar and D. Sameera and Garapati, Yugandhar and D. Krishnamadhuri and Bethu, Srikanth;2022;"Development of Deep Learning models of Internet of Things (IoT) enclosures with limited resources are difficult because Both Quality of Results are difficult to achieve&nbsp;- QoR as follows two models, DNN Model, and Inference Accuracy and Quality of Services such as power consumption, throughput, and latency. Currently, the development of DNN models is often separated from deploying them to IoT devices, which leads to the most effective solution. If there are many records that represent objects of substantially the same class (face, human body, etc.), you can apply frames to each object of this class. To achieve this, use an independent representation to distinguish between appearance and progress data. Deep fake detection is achieved by using a novel, lightweight Deep Learning method on the IoT platform that is memory-efficient and lightweight.&nbsp;It is carried out in two different stages. The first phase of the deep fake test aims to implement a method of extracting images from a video and using them in conjunction with a Deep Neural Network to implement a test for face animation.&nbsp;It has been reported that the impact of the background elimination has been reported before the background subtraction. Here the Trans GAN model is used for the image classification. In the second phase, the work can be recorded and executed by the IOT device that can record live video streams and then detect activity involved in live video. An activity detection prototype based on IoT devices with small processing power is presented. This prototype provides improvements to the system, extending its application in various ways to improve portability, networking, and other equipment capabilities. The proposed architecture will be evaluated against four highly competitive object detection benchmarking tasks CIFAR10, CIFAR100, SVHN, and ImageNet.";Object detection, GAN, Face animation, Deep Fake;ACM;WAHR
12;Can Deepfakes be created on a whim?;Mehta, Pulak and Jagatap, Gauri and Gallagher, Kevin and Timmerman, Brian and Deb, Progga and Garg, Siddharth and Greenstadt, Rachel and Dolan-Gavitt, Brendan;2023;"Recent advancements in machine learning and computer vision have led to the proliferation of Deepfakes. As technology democratizes over time, there is an increasing fear that novice users can create Deepfakes, to discredit others and undermine public discourse. In this paper, we conduct user studies to understand whether participants with advanced computer skills and varying level of computer science expertise can create Deepfakes of a person saying a target statement using limited media files. We conduct two studies; in the first study (n = 39) participants try creating a target Deepfake in a constrained time frame using any tool they desire. In the second study (n = 29) participants use pre-specified deep learning based tools to create the same Deepfake. We find that for the first study, of the participants successfully created complete Deepfakes with audio and video, whereas for the second user study, of the participants were successful in stitching target speech to the target video. We further use Deepfake detection software tools as well as human examiner-based analysis, to classify the successfully generated Deepfake outputs as fake, suspicious, or real. The software detector classified of the Deepfakes as fake, whereas the human examiners classified of the videos as fake. We conclude that creating Deepfakes is a simple enough task for a novice user given adequate tools and time; however, the resulting Deepfakes are not sufficiently real-looking and are unable to completely fool detection software as well as human examiners.";deepfakes, generative models, video synthesis;ACM;FALSCH
15;Recapture Detection to Fight Deep Identity Theft;Trabelsi, Anis and Pic, Marc and Dugelay, Jean-Luc;2023;The progress made in deep learning has allowed the deployment of more powerful biometric authentication systems instead of traditional ones based on passwords or PIN codes. Facial recognition is widely used on smartphones to grant user access. However, advances in deep learning also improve methods for doctoring images and videos. A fraudulent user can use these methods to steal the identity of another person. It is very easy for impostors to present to the smartphone an image or video of the victim's face displayed on another screen. In this paper, we describe the security risks when a facial recognition system is attacked by presenting an image, a video or an interactive deepfake displayed on a screen. We also present a deep learning-based method to detect this kind of attack.;recaptured image detection, identity theft, face anti-spoofing, e-KYC, digital image forensics;ACM;FALSCH
20;Deepfake Video Detection via Predictive Representation Learning;Ge, Shiming and Lin, Fanzhao and Li, Chenyu and Zhang, Daichi and Wang, Weiping and Zeng, Dan;2022;Increasingly advanced deepfake approaches have made the detection of deepfake videos very challenging. We observe that the general deepfake videos often exhibit appearance-level temporal inconsistencies in some facial components between frames, resulting in discriminative spatiotemporal latent patterns among semantic-level feature maps. Inspired by this finding, we propose a predictive representative learning approach termed Latent Pattern Sensing to capture these semantic change characteristics for deepfake video detection. The approach cascades a Convolution Neural Network-based encoder, a ConvGRU-based aggregator, and a single-layer binary classifier. The encoder and aggregator are pretrained in a self-supervised manner to form the representative spatiotemporal context features. Then, the classifier is trained to classify the context features, distinguishing fake videos from real ones. Finally, we propose a selective self-distillation fine-tuning method to further improve the robustness and performance of the detector. In this manner, the extracted features can simultaneously describe the latent patterns of videos across frames spatially and temporally in a unified way, leading to an effective and robust deepfake video detector. Extensive experiments and comprehensive analysis prove the effectiveness of our approach, e.g., achieving a very highest Area Under Curve (AUC) score of 99.94% on FaceForensics++ benchmark and surpassing 12 states of the art at least 7.90%@AUC and 8.69%@AUC on challenging DFDC and Celeb-DF(v2) benchmarks, respectively.;video understanding, deep learning, representation learning, Deepfake video detection;ACM;WAHR
21;Human Perception of Audio Deepfakes;"M\""{u}ller, Nicolas M. and Pizzi, Karla and Williams, Jennifer";2022;The recent emergence of deepfakes has brought manipulated and generated content to the forefront of machine learning research. Automatic detection of deepfakes has seen many new machine learning techniques. Human detection capabilities, however, are far less explored. In this paper, we present results from comparing the abilities of humans and machines for detecting audio deepfakes used to imitate someone's voice. For this, we use a web-based application framework formulated as a game. Participants were asked to distinguish between real and fake audio samples. In our experiment, 410 unique users competed against a state-of-the-art AI deepfake detection algorithm for 13229 total of rounds of the game. We find that humans and deepfake detection algorithms share similar strengths and weaknesses, both struggling to detect certain types of attacks. This is in contrast to the superhuman performance of AI in many application areas such as object detection or face recognition. Concerning human success factors, we find that IT professionals have no advantage over non-professionals but native speakers have an advantage over non-native speakers. Additionally, we find that older participants tend to be more susceptible than younger ones. These insights may be helpful when designing future cybersecurity training for humans as well as developing better detection algorithms.;presentation attack, human perception, games, deepfake, audio spoofing;ACM;FALSCH
58;DeepFake detection method based on multi-scale interactive dual-stream network;Ziyuan Cheng and Yiyang Wang and Yongjing Wan and Cuiling Jiang;2024;"DeepFake face forgery has a serious negative impact on both society and individuals. Therefore, research on DeepFake detection technologies is necessary. At present, DeepFake detection technology based on deep learning has achieved acceptable results on high-quality datasets; however, its detection performance on low-quality datasets and cross-datasets remains poor. To address this problem, this paper presents a multi-scale interactive dual-stream network (MSIDSnet). The network is divided into spatial- and frequency-domain streams and uses a multi-scale fusion module to capture both the facial features of images that have been manipulated in the spatial domain under different circumstances and the fine-grained high-frequency noise information of forged images. The network fully integrates the features of the spatial- and frequency-domain streams through an interactive dual-stream module and uses vision transformer (ViT) to further learn the global information of the forged facial features for classification. Experimental results confirm that the accuracy of this method reached 99.30?% on the high-quality dataset Celeb-DF-v2, and 95.51?% on the low-quality dataset FaceForensics++. Moreover, the results of the cross-dataset experiments were superior to those of the other comparison methods.";DeepFake detection, Multi-scale fusion, Interactive dual-stream, High-frequency noise;ScienceDirect;WAHR
64;STB-VMM: Swin Transformer based Video Motion Magnification;Ricard Lado-Roigé and Marco A. Pérez;2023;The goal of video motion magnification techniques is to magnify small motions in a video to reveal previously invisible or unseen movement. Its uses extend from bio-medical applications and deepfake detection to structural modal analysis and predictive maintenance. However, discerning small motion from noise is a complex task, especially when attempting to magnify very subtle, often sub-pixel movement. As a result, motion magnification techniques generally suffer from noisy and blurry outputs. This work presents a new state-of-the-art model based on the Swin Transformer, which offers better tolerance to noisy inputs as well as higher-quality outputs that exhibit less noise, blurriness, and artifacts than prior-art. Improvements in output image quality will enable more precise measurements for any application reliant on magnified video sequences, and may enable further development of video motion magnification techniques in new technical fields.;Computer vision, Deep learning, Swin Transformer, Motion magnification, Image quality assessment;ScienceDirect;FALSCH
66;Voice spoofing detector: A unified anti-spoofing framework;Ali Javed and Khalid Mahmood Malik and Hafiz Malik and Aun Irtaza;2022;Voice controlled systems (VCS) in Internet of Things (IoT), speaker verification systems, voice-based biometrics, and other voice-assistant-enabled systems are vulnerable to different spoofing attacks i.e., replay, cloning, cloned-replay, etc. VCS are not only susceptible to these attacks in a non-network environment, but they are also vulnerable to multi-order spoofing attacks in networked IoT. Additionally, deepfakes with artificially generated audio pose a great threat to the all systems having voice-interfaces. Most of the existing countermeasures against these voice spoofing attacks work for only one specific attack (e.g. voice replay) and fail to generalize this for other classes of spoofing attacks. Additionally, generalization is also crucial for cross-corpora evaluation. Thus, there exists a need to develop a unified voice anti-spoofing framework capable of detecting multiple spoofing attacks. This work presents a unified anti-spoofing framework that uses novel (ATCoP-GTCC) features to combat the variety of voice spoofing attacks. The proposed novel acoustic-ternary co-occurrence patterns (ATCoP) encode the co-occurrence of similar patterns between the center and neighboring samples. Our experiments demonstrate that ATCoP can better capture the microphone induced distortions in replays, unnatural prosody and algorithmic artifacts in cloned samples, and both the distortions and artifacts in cloned-replays including compression on multi-hop attacks in the spoofing samples. The performance of ATCoP could be further enhanced by the Gammatone cepstral coefficients. To evaluate the effectiveness of the proposed anti-spoofing system for multi-order replay and cloned-replay attacks detection, we created a diverse voice spoofing detection corpus (VSDC) containing multi-order replay and cloned-replay audios against the bonafide and cloned audio recordings, respectively. Experimental results obtained on VSDC, ASVspoof 2019, Googles LJ Speech, and YouTube deepfakes datasets illustrate the effectiveness of the proposed system in terms of accurate detection for a variety of voice spoofing attacks.;Acoustic ternary co-occurrence patterns, AI for multimedia security, AI for voice-based biometrics in IoT, Anti-spoofing against multiple attack vectors, Deepfakes, Voice spoofing detection;ScienceDirect;FALSCH
72;Deep Fakes in Healthcare: How Deep Learning Can Help to Detect Forgeries;Alaa Alsaheel and Reem Alhassoun and Reema Alrashed and Noura Almatrafi and Noura Almallouhi and Saleh Albahli;2023;With the increasing use of deep learning technology, there is a growing concern over creating deep fake images and videos that can potentially be used for fraud. In healthcare, manipulating medical images could lead to misdiagnosis and potentially life-threatening consequences. Therefore, the primary purpose of this study is to explore the use of deep learning algorithms to detect deep fake images by solving the problem of recognizing the handling of samples of cancer and other diseases. Therefore, this research proposes a framework that leverages state-of-the-art deep convolutional neural networks (CNN) and a large dataset of authentic and deep fake medical images to train a model capable of distinguishing between authentic and fake medical images. Specifically, the paper trained six CNN models, namely, ResNet101, ResNet50, DensNet121, DenseNet201, MobileNetV2, and MobileNet. These models had trained using 2000 samples over three classes: Untampered, False-Benign, and False-Malicious, and compared against several state-of-the-art deep fake detection models. The proposed model enhanced ResNet101 by adding more layers, achieving a training accuracy of 99%. The findings of this study show near-perfect accuracy in detecting instances of tumor injections and removals.;Deep learning, image processing, medical imaging, artificial intelligence;ScienceDirect;WAHR
78;Using deep learning techniques and genetic-based feature extraction for presentation attack mitigation;John Jenkins and Kaushik Roy and Joseph Shelton;2020;"Abstract
Biometric authentication systems are becoming more prevalent for commercial use with computers and smart devices. Biometric systems also have several vulnerable points that can be exploited by a hacker to gain unauthorized access to a system. Replay attacks focus on capturing feature extractors (FEs) during transmission, decrypting, and replaying for illegal access. The Genetic and Evolutionary Feature Extraction (GEFE) technique, developed at North Carolina A&T State University, recently showed promising results in mitigating replay attacks in combination with a feature selection algorithm. Biometric-based presentation attacks, the focus of this work, is another biometric system vulnerability primarily focused on presenting a biometric sample of quality to illegally gain access to secured data. Recently, deep learning techniques to mitigate presentation attacks have shown promising results. However, the accuracy of deep learning-based biometric presentation attack detection (PAD) methods are limited by the quality of the samples provided. In absence of large sets of original biometric sample data, data augmentation has been shown to be successful in generating synthetic biometric image data and improving the performance of deep learning techniques applied. The novelty of this paper lies in the following two aspects: First, a data augmentation technique with Generative Adversarial Networks (GANs) is used to generate comparative synthetic (spoofing) dataset. With the proliferation of deep fakes in media, this technique should provide insight on the GAN technique often used. Once properly trained, the synthetic images are used to create spoofing datasets. Second, the GEFE technique is used in combination with the GANs to generate improved anti-spoofing feature extractors optimized to mitigate presentation attacks. The combination of GEFE and GANs is used to identify those discriminative biometric features used to mitigate synthetic presentation attacks. The GEFE ?+ ?GAN technique outperforms the LBP and GEFE techniques alone in overall identification and verification results on spoofing datasets.";, , ;ScienceDirect;WAHR
87;A study on data augmentation in voice anti-spoofing;Ariel Cohen and Inbal Rimon and Eran Aflalo and Haim H. Permuter;2022;In this paper we perform an in depth study of how data augmentation techniques improve synthetic or spoofed audio detection. Specifically, we propose methods to deal with channel variability, different audio compressions, different bandwidths and unseen spoofing attacks. These challenges, have all been shown to significantly degrade the performance of audio based systems and anti spoofing systems. Our results are based on the ASVspoof 2021 challenge, in the Logical Access (LA) and Deep Fake (DF) categories. Our study is Data-Centric, meaning that the models are fixed and we significantly improve the results by manipulating the data. We introduce two forms of data augmentation - compression augmentation for the DF part, and compression and channel augmentation for the LA part. In addition, we introduce a double sided log spectrogram feature design that improves the results significantly by centering the sub-bands of interest, where the discriminating spoofing artifacts can be localized. Furthermore, a new type of online data augmentation, SpecAverage, is introduced. This method includes masking the audio features with their average value in order to improve generalization. Our best single system and fusion schemes both achieve state of the art performance in the DF category, with an EER of 15.46% and 14.27%, respectively. Our best system for the LA task reduced the best baseline EER by 50% and the min t-DCF by 16%. Our techniques to deal with spoofed data from a wide variety of distributions can be replicated and can help anti spoofing and speech based systems enhance their results.;ASVspoof 2021, Audio data augmentation, Data-centric AI, SpecAugment, Voice anti spoofing, Voice deep fake;ScienceDirect;FALSCH
92;Audio-deepfake detection: Adversarial attacks and countermeasures;Mouna Rabhi and Spiridon Bakiras and Roberto {Di Pietro};2024;Audio has always been a powerful resource for biometric authentication: thus, numerous AI-based audio authentication systems (classifiers) have been proposed. While these classifiers are effective in identifying legitimate human-generated input their security, to the best of our knowledge, has not been explored thoroughly when confronted with advanced attacks that leverage AI-generated deepfake audio. This issue presents a serious concern regarding the security of these classifiers because, e.g., samples generated using adversarial attacks might fool such classifiers, resulting in incorrect classification. In this study, we prove the point: we demonstrate that state-of-the-art audio deepfake classifiers are vulnerable to adversarial attacks. In particular, we design two adversarial attacks on a state-of-the-art audio-deepfake classifier, i.e., the Deep4SNet classification model, which achieves 98.5% accuracy in detecting fake audio samples. The designed adversarial attacks11The code of the attacks will be released open-source in the camera ready. leverage a generative adversarial network architecture and reduce the detectors accuracy to nearly 0%. In particular, under graybox attack scenarios, we demonstrate that when starting from random noise, we can reduce the accuracy of the state-of-the-art detector from 98.5% to only 0.08%. To mitigate the effect of adversarial attacks on audio-deepfake detectors, we propose a highly generalizable, lightweight, simple, and effective add-on defense mechanism that can be implemented in any audio-deepfake detector. Finally, we discuss promising research directions.;Authentication, Adversarial attacks, Audio deepfake, Fake voice detection, GAN, Biometrics, Security;ScienceDirect;FALSCH
125;Acoustic features analysis for explainable machine learning-based audio spoofing detection;Carmen Bisogni and Vincenzo Loia and Michele Nappi and Chiara Pero;2024;The rapid evolution of synthetic voice generation and audio manipulation technologies poses significant challenges, raising societal and security concerns due to the risks of impersonation and the proliferation of audio deepfakes. This study introduces a lightweight machine learning (ML)-based framework designed to effectively distinguish between genuine and spoofed audio recordings. Departing from conventional deep learning (DL) approaches, which mainly rely on image-based spectrogram features or learning-based audio features, the proposed method utilizes a diverse set of hand-crafted audio features  such as spectral, temporal, chroma, and frequency-domain features  to enhance the accuracy of deepfake audio content detection. Through extensive evaluation and experiments on three well-known datasets, ASVSpoof2019, FakeAVCelebV2, and an In-The-Wild database, the proposed solution demonstrates robust performance and a high degree of generalization compared to state-of-the-art methods. In particular, our method achieved 89% accuracy on ASVSpoof2019, 94.5% on FakeAVCelebV2, and 94.67% on the In-The-Wild database. Additionally, the experiments performed on explainability techniques clarify the decision-making processes within ML models, enhancing transparency and identifying crucial features essential for audio deepfake detection.;Deepfake audio, Deepfake detection, Audio spoofing, Explainable AI, Acoustic features;ScienceDirect;FALSCH
150;Audio Deepfake Detection by using Machine and Deep Learning;"H. H. Kilinc; F. Kaledibi";2023;Fake voices are one of the hottest topics in cyber security, forensics, and social media. There are a variety of usage scenarios, from speech disorders to fake news to telephone and financial fraud. With products using artificial intelligence technology such as Google Audio LM, it is possible to produce realistic, well-structured, and consistent sound sequences. These products, although synthetic, can accurately replicate intonation, accents, and other unique features by mimicking the human voice. A solution using machine and deep learning methods to recognize fake voices is proposed. In the feature extraction stage, Mel-frequency cepstral coefficients (MFCCs) are used. Then these features are classified using machine and deep learning-based models. According to the results obtained, the sound is judged to be real or fake.;"Audio Deepfake;MFCC;Feature Extraction;Fake Audio Detection";IEE;FALSCH
268;Deepfake Audio Detection Using Spectrogram-based Feature and Ensemble of Deep Learning Models;"L. Pham; P. Lam; T. Nguyen; H. Nguyen; A. Schindler";2024;In this paper, we propose a deep-learning-based system for the task of deepfake audio detection. This work is a part of the proposed toolchain for speech analysis in EUCINF (EUropean Cyber and INFormation) project, which is an European project with multiple partners in Europe. In particular, the raw input audio is first transformed into various spectrograms using three transformation methods of Short-time Fourier Transform (STFT), Constant-Q Transform (CQT), Wavelet Transform (WT) combined with different auditory- based filters of Mel, Gammatone, linear filters (LF), and discrete cosine transform (DCT). Given the spectrograms, we evaluate a wide range of classification models based on three deep learning approaches. The first approach is to train the spectrograms using our proposed baseline models of CNN-based model (CNN- baseline), RNN-based model (RNN-baseline), C-RNN model (C-RNN baseline). Meanwhile, the second approach is to apply the transfer learning from computer vision models such as ResNet- 18, MobileNet-V3, EfficientNet-BO, DenseNet-121, SuffleNet- V2, Swint, Convnext- Tiny, GoogLeNet, MNASsnet, and Reg- Net. In the third approach, we leverage the state-of-the-art audio pre-trained models of Whisper, Seamless, Speechbrain, and Pyannote to extract audio embed dings from the input spectrograms. Then, the audio embed dings are explored by a Multilayer perceptron (MLP) model to detect fake or real audio samples. Finally, high-performance deep learning models from these approaches are fused to achieve the best performance. We evaluated our proposed models on ASVspoof 2019 benchmark dataset. Our best ensemble model achieved an Equal Error Rate (EER) of 0.03, which is highly competitive to top-performing systems in the ASVspoofing 2019 challenge. Experimental results also highlight the potential of selective spectrograms and deep learning approaches to enhance model performance on the task of audio deepfake detection.;"Items- deepfake audio;spectrogram;feature extraction;classification model";IEE;FALSCH
282;Integrating Audio-Visual Features For Multimodal Deepfake Detection;"S. Muppalla; S. Jia; S. Lyu";2023;Deepfakes are AI-generated media in which an image or video has been digitally modified. The advancements made in deepfake technology have led to privacy and security issues. Most deepfake detection techniques rely on the detection of a single modality. Existing methods for audio-visual detection do not always surpass that of the analysis based on single modalities. Therefore, this paper proposes an audio visual based method for deepfake detection, which integrates fine-grained deepfake identification with binary classification. We categorize the samples into four types by combining labels specific to each single modality. This method enhances the detection under intra-domain and cross-domain testing.;"Deepfake detection;Multi-modality deepfakes;Audio-visual feature learning";IEE;FALSCH
293;Voice Analysis for Detecting Artificial Speech and Predicting Gender and Age: A Literature Survey;"R. Bharadwaj; R. Dugad; A. Gile; G. Patil; S. Hatyalikar";2023;This article surveys the literature on voice analysis. The present invention relates to a method and system for analyzing voice data to detect artificial speech then demodulate it to original speech and predict the gender and age of the speaker if the input speech is not fake. The system includes a voice analysis module that processes the voice data and extracts features from the voice signals. The extracted features are then compared with pre-stored reference features to determine the authenticity of the voice signal and the gender and age of the speaker.;"Voice Detection;Audio Processing;deep learning;gender voice prediction;neural networks";IEE;FALSCH
328;Bot or Human? Detection of DeepFake Text with Semantic, Emoji, Sentiment and Linguistic Features;"A. T. Y. Chong; H. N. Chua; M. B. Jasser; R. T. K. Wong";2023;Detecting machine-generated text (MGT), also known as Deepfake text, has become increasingly important in Artificial Intelligence (AI) age and social media platforms. With the proliferation of MGT and the potential consequences of its dissemination, there is a pressing need to develop effective methods for distinguishing between MGT and human-written text (HWT). Our research aim has two-fold: firstly, to examine the inherent differences between MGT and HWT on Twitter, and secondly, to develop a classifier specifically designed for MGT detection on the platform. This classifier utilizes contextualized text embeddings as its foundation while considering additional linguistic features, sentiment features, and emoji embeddings. Our experimental results demonstrate that incorporating additional features enhances the model's ability to detect MGT. Combining fine-tuned BERT embeddings with emoji and linguistic features using a multi-layer perceptron classifier achieves the highest accuracy rate of 88.3%. Our analysis reveals distinct characteristics of MGT compared to HWT, including differences in engagement behavior, linguistic patterns, named entities, sentiment expressions, and text perplexity. Our research contributes to the field of MGT detection by offering a comprehensive approach that combines semantic text embeddings with supplementary features. The proposed model provides a significant step forward in addressing the challenge of Deepfake text detection.;"Artificial Intelligence;Machine Learning;Data Mining;Deepfake Detection;Feature Engineering";IEE;FALSCH
335;Audio Deepfake Detection Using Deep Learning;"R. Anagha; A. Arya; V. H. Narayan; S. Abhishek; T. Anjali";2023;The capacity to identify real audio recordings from their modified counterparts is essential in the age of sophisticated digital manipulation for maintaining security and trust in a vari- ety of applications, from media forensics to voice authentication systems. This research aims to create a deep learning model that can distinguish between authentic and altered audio files, with an emphasis on identifying audio deepfakes. The study uses Mel spectrogram representations and data augmentation techniques to effectively extract features from the ASV spoof 2019 dataset and train models. Convolutional neural networks (CNNs) comprising a number of layers, including convolutional, pooling, batch normalization, ReLU activation, dropout, global average pooling, and a dense classification layer are used as the foundation of the design. The Adam optimizer is used to optimize the model once it has been trained using binary cross-entropy loss, and a variety of metrics, such as accuracy, F1 score, ROC curve, and AUC, are used to track its performance. By making it easier to identify audio deepfakes, this project will ultimately increase the security and integrity of audio data in the digital world.;"Audio Deepfake Detection;Deep Learning;Convolutional Neural Network (CNN);Mel Spectrogram;ASVspoof 2019 Dataset;Binary Classification;Feature Extraction;Perfor- mance Metrics";IEE;FALSCH
347;Deepfake Speech Detection Through Emotion Recognition: A Semantic Approach;"E. Conti; D. Salvi; C. Borrelli; B. Hosler; P. Bestagini; F. Antonacci; A. Sarti; M. C. Stamm; S. Tubaro";2022;In recent years, audio and video deepfake technology has advanced relentlessly, severely impacting peoples reputation and reliability. Several factors have facilitated the growing deepfake threat. On the one hand, the hyper-connected society of social and mass media enables the spread of multimedia content worldwide in real-time, facilitating the dissemination of counterfeit material. On the other hand, neural network-based techniques have made deepfakes easier to produce and difficult to detect, showing that the analysis of low-level features is no longer sufficient for the task. This situation makes it crucial to design systems that allow detecting deepfakes at both video and audio levels. In this paper, we propose a new audio spoofing detection system leveraging emotional features. The rationale behind the proposed method is that audio deepfake techniques cannot correctly synthesize natural emotional behavior. Therefore, we feed our deepfake detector with high-level features obtained from a state-of-the-art Speech Emotion Recognition (SER) system. As the used descriptors capture semantic audio information, the proposed system proves robust in cross-dataset scenarios outperforming the considered baseline on multiple datasets.;"deepfake;audio forensics;deep learning";IEE;FALSCH
376;Speech Audio Deepfake Detection via Convolutional Neural Networks;"L. P. Valente; M. M. S. de Souza; A. M. D. Rocha";2024;The production of artificial media content brings on ethical, legal and social implications for journalism, education, entertainment and industry. Software tools are currently available for anyone who intent to maliciously generate or tamper with digital audio voices. In this context, detecting voice authenticity is important to avoid the consequences of its criminal use. Here, we propose the application of convolutional neural networks (CNN) and Mel spectograms in detection of artificially generated voices. Supervised experiments with speech samples signals, collected from several voice datasets, were conducted to find the best CNN topology that performs the detection, in terms of accuracy, regardless of the language spoken. The best accuracy scores found are: 99% for the FoR dataset, 94% for the ASV and 98% for the WaveFake. Training the model with all datasets together, and testing with individual datasets, yields accuracies of 98% for the FoR base, 92% for the ASV and 96% for WaveFake. These results are compatible with those found in state-of-the-art, proving the viability of the model.;"Forensics analysis;Deepfake;Voice cloning;Computer vision;Machine learning;Deep learning";IEE;FALSCH
387;Deepfake Audio Detection: A Deep Learning Based Solution for Group Conversations;"R. L. M. A. P. C. Wijethunga; D. M. K. Matheesha; A. A. Noman; K. H. V. T. A. De Silva; M. Tissera; L. Rupasinghe";2020;The recent advancements in deep learning and other related technologies have led to improvements in various areas such as computer vision, bio-informatics, and speech recognition etc. This research mainly focuses on a problem with synthetic speech and speaker diarization. The developments in audio have resulted in deep learning models capable of replicating natural-sounding voice also known as text-to-speech (TTS) systems. This technology could be manipulated for malicious purposes such as deepfakes, impersonation, or spoofing attacks. We propose a system that has the capability of distinguishing between real and synthetic speech in group conversations.We built Deep Neural Network models and integrated them into a single solution using different datasets, including but not limited to Urban-Sound8K (5.6GB), Conversational (12.2GB), AMI-Corpus (5GB), and FakeOrReal (4GB). Our proposed approach consists of four main components. The speech-denoising component cleans and preprocesses the audio using Multilayer- Perceptron and Convolutional Neural Network architectures, with 93% and 94% accuracies accordingly. The speaker diarization was implemented using two different approaches, Natural Language Processing for text conversion with 93% accuracy and Recurrent Neural Network model for speaker labeling with 80% accuracy and 0.52 Diarization-Error-Rate. The final component distinguishes between real and fake audio using a CNN architecture with 94 % accuracy. With these findings, this research will contribute immensely to the domain of speech analysis.;"Deep Neural Networks;Natural Language Processing;Speaker Diarization;Deepfake;Deep learning";IEE;FALSCH
416;Document Image Forgery Detection Based on Deep Learning Models;"P. Yang; W. Fang; F. Zhang; L. Bai; Y. Gao";2022;With the improvement of the communication speed and the popularization of the Internet, images have become the most common information medium in life. At the same time, the adverse effects of forged images in the media, credit investigation, finance and academic fields are becoming more and more significant. Therefore, in recent years, the research on forged image identification algorithms has been active worldwide. Image forgery has different classification methods. According to whether the forgery uses deep learning methods, it can be divided into deep forged images and traditional forged images. It can also be divided into ordinary image forged and document image forged according to whether the image is a text image. Different forgery methods will leave different forgery traces in the image, corresponding to different forgery identification methods. Aiming at document forgery images, this paper proposes a forgery detection algorithm based on deep learning and fusion of error level analysis (ELA) information. Compared with the previous forgery identification algorithms, the algorithm in this paper can not only identify whether the document image is forged, but can also locate the forged text area. The algorithm proposed in this paper supports the detection of document image forgery generated by cutting, copying, erasing and deep learning methods. The detection algorithm of this paper participated in the fifth forgery detection competition of Ali Tianchi and won the 32nd place among 1470 participating teams.;"Document Image Forgery Identification;Deep Learning;ELA Image Processing";IEE;FALSCH
847;Detection of AI-Generated Images From Various Generators Using Gated Expert Convolutional Neural Network;"R. Ahmad Fattah Saskoro; N. Yudistira; T. Noor Fatyanosa";2024;The rapid advancement of artificial intelligence (AI), particularly in text-to-image generative models, has led to a proliferation of synthetic images. This progress, while remarkable, raises concerns about misuse in fraudulent activities. To address this issue, we propose a Convolutional Neural Network (CNN)-based approach for classifying AI-generated images from multiple generators. We introduce a gated CNN model that leverages mixed datasets for improved training efficiency and performance. This approach eliminates the need for extensive tuning with each new dataset and mitigates the risk of catastrophic forgetting. Our experiments demonstrate that the gated CNN model slightly outperforms traditional single CNN models, providing a more robust solution for identifying AI-generated images. This paper presents a comprehensive comparison of methods and offers insights into enhancing the classification of AI-generated images.;"AI-generated images;CNN;gated network;image classification";IEE;WAHR
868;Detection of Diffusion Model-Generated Faces by Assessing Smoothness and Noise Tolerance;"B. Liu; B. Liu; M. Ding; T. Zhu";2024;The fast growth of artificial intelligence (AI) raises much concern about the misinformation brought by AI-generated content (AIGC), especially Deepfake techniques that generate fake human faces. The recent development of Diffusion Models (DMs) moves another critical step forward to generate high-resolution and realistic human faces, which has become a challenge for existing Deepfake detectors. In this paper, we propose a DM-generated image detector by looking into the generation pipeline of DMs and the details of DM-generated images. The detector is based on the observation that DM-generated human faces show over-smooth textures and do not contain details as real human faces. Through a comprehensive analysis of DM-generated faces in spatial and frequency domains, we noticed that the over-smoothness improves the tolerance of Gaussian noise since excessive smoothness mitigates some of the impact of noise. Inspired by the observations, we propose a Deepfake detector capable of recognizing challenging DM-generated faces. We mainly propose the Noise Residual Unit (NRD) in our framework to collect the frequency response of images to Gaussian noise as distinctive features for classification. In detail, for an input face image, we add Gaussian noise to it and get the noise-degraded image. Then, the NRU generates the Noise Residual Image (NRI) by calculating the residual of the high-pass-filtered original image and the high-pass-filtered degraded image. The NRI indicates the high-frequency impact brought by the Gaussian noise and, therefore, suggests the tolerance of the original image to noise degradation. The original image and NRI are encoded and fused to obtain the joint representation, which is then fed to a classifier to predict the binary label. We conducted comprehensive experiments to evaluate the effectiveness of the proposed detector. The results indicate that our proposed detector achieves state-of-the-art detection performance on DM-generated faces and generalizes well to unseen DM-generated and GAN-generated face datasets.;"Deepfake detection;diffusion models;frequency analysis";IEE;WAHR
873;A Multi-Layer Capsule-based Forensics Model for Fake Detection of Digital Visual Media;"S. S. Khalil; S. M. Youssef; S. N. Saleh";2021;The dangers generated from synthesized multimedia are increasing every day. The creation of the so-called Deepfakes multimedia is vastly evolving, making the detection task harder every day. Researchers and corporations are interested in exploring the technology limits and are coming up with new tools every year to create more robust fake media. In this paper, a new enhanced fake video detection model is introduced addressing many of the face-swapping threats and the low generalization problem. A preprocessing stage is proposed to minimize the noise in the data to enhance their quality. The proposed architecture uses a modified application of capsule neural networks (CapsNet) with an enhanced routing technique. It does not require a lot of training data and generates a small number of training parameters making it fast to build. The model was trained and tested using the DFDC-P dataset and the results have proven that it outperformed other detectors in terms of detection recall, weighted precision, and F1 score.;"deepfake detection;capsule network;capsnet";IEE;WAHR
874;Detecting Synthetic Speech Manipulation in Real Audio Recordings;"M. Hafizur Rahman; M. Graciarena; D. Castan; C. Cobo-Kroenke; M. McLaren; A. Lawson";2022;"Recent advances in artificial speech and audio technologies have improved the abilities of deep-fake operators to falsify media and spread malicious misinformation. Anyone with limited coding skills can use freely available speech synthesis tools to create convincing simulations of influential speakers voices with the malicious intent to distort the original message. With the latest technology, malicious operators do not have to generate an entire audio clip; instead, they can insert a partial manipulation or a segment of synthetic speech into a genuine audio recording to change the entire context and meaning of the original message. Detecting these insertions is especially challenging because partially manipulated audio can more easily avoid synthetic speech detectors than entirely fake messages can. This paper describes a potential synthetic speech detection system based on the x-ResNet architecture with a probabilistic linear discriminant analysis (PLDA) backend. Experimental results suggest that the PLDA backend results in a 25% average error reduction among partially synthesized datasets over a non-PLDA baseline.";"partial synthetic speech;synthetic speech detection;anti-spoof;x-ResNet;PLDA";IEE;FALSCH
878;A Dataless FaceSwap Detection Approach Using Synthetic Images;"A. Jain; N. Memon; J. Togelius";2022;Face swapping technology used to create Deepfakes has advanced significantly over the past few years and now enables us to create realistic facial manipulations. Current deep learning algorithms to detect deepfakes have shown promising results, however, they require large amounts of training data, and as we show they are biased towards a particular ethnicity. We propose a deepfake detection methodology that eliminates the need for any real data by making use of synthetically generated data using Style-GAN3. This not only performs at par with the traditional training methodology of using real data but it shows better generalization capabilities when finetuned with a small amount of real data. Furthermore, this also reduces biases created by facial image datasets that might have sparse data from particular ethnicities. To promote reproducibility the code base has been made publicly available 11https://github.com/anubhav1997/youneednodataset;;IEE;WAHR
880;DFP-Net: An explainable and trustworthy framework for detecting deepfakes using interpretable prototypes;"F. Khalid; A. Javed; K. M. Malik; A. Irtaza";2023;The rise of deepfake videos poses a serious threat to the authenticity of visual media, as they have a potential to manipulate public opinion, mislead individuals or groups, harm reputation, etc. Traditional methods for detecting deepfakes rely on deep learning models, which lack transparency and interpretability. To gain the confidence of forensic experts in AI-based deepfakes detector, we present a novel DFP-Net for detecting deepfakes using interpretable and explainable prototypes. Our method makes use of the power of prototype-based learning to generate a set of representative images that capture the essential features of genuine and deepfake images. These prototypes are then used to explain our models decision-making process and to provide insights into the features most relevant for deepfake detection. We then use these prototypes to train a classification model that can detect deepfakes accurately and with high interpretability. To further improve the interpretability of our method, we also utilize the Grad-CAM technique to generate heatmaps that highlight the regions of the image that contribute the most towards the decision of the model. These heatmaps can be used to explain the reasoning behind the models decision and provide insights into the visual cues that distinguish deepfakes from real images. Experimental results on a large-scale FaceForensics++, Celeb-DF and DFDC-P datasets demonstrate that our method achieves state-of-the-art performance in deepfakes detection. Moreover, the interpretability and explainability of our method make it more trustworthy to forensic experts by allowing them to understand how the model works and makes predictions.;"Deepfakes detection;DFP-Net;Interpretable prototypes;Explainable AI;FaceForensics++";IEE;WAHR
892;Exposing DeepFakes Using Convolutional Neural Networks and Transfer Learning Approaches;"S. Suratkar; F. Kazi; M. Sakhalkar; N. Abhyankar; M. Kshirsagar";2020;Advancements in Artificial Intelligence - oriented computing power and the ever-growing reach of social media have proven to be catalysts in emergence and spread of a new vein of AI generated fake videos known as 'DeepFake' videos. Such videos are synthesized using generative machine learning models like Generative Adversarial Networks or Variational AutoEncoders and they can achieve high degrees of realism. Spread of sensitive political or obscene content in form of such videos may lead to social distress to the target entity(s). This paper presents a study pertinent to the detection of DeepFake videos using Convolutional Neural Networks (CNNs) with transfer learning. A comparative study of the performance of various models in the detection of tampered videos has been presented. These models are trained (fine-tuned) and tested on a custom dataset encompassing randomly selected labelled frames from videos in the DeepFake Detection Dataset by Google AI and FaceForensics++ dataset.;"Convolutional Neural Networks;Generative Adversarial Networks;Transfer Learning;Visual Geometry Group;DenseNet;Xception;Inception V3";IEE;WAHR
900;Art of Detection: Custom CNN and VGG19 for Accurate Real Vs Fake Image Identification;"H. V; K. P; M. A";2023;In the current digital age, the ability to distinguish between real and manipulated images has become crucial due to the proliferation of doctored images. This research aims to address this challenge by employing two distinct neural network architectures: a custom Convolutional Neural Network (CNN) and the pre-trained VGG19 model for the classification of real versus fake images. Experimental results reveal that the custom CNN achieved a noteworthy accuracy of 94.46%, outperforming the VGG19 model, which secured an accuracy of 84.24%. Such findings suggest that while pre-trained models like VGG19 bring significant value to image classification tasks, a tailored CNN can offer superior performance for specialized tasks such as detecting image authenticity. This study provides a foundation for further exploration in image forensics, emphasizing the importance of model selection and optimization in combating digital image manipulations.;"Detecting image authenticity;Convolutional Neural Network (CNN);VGG19 Model;Image Classification";IEE;WAHR
905;Lip Sync Matters: A Novel Multimodal Forgery Detector;"S. A. Shahzad; A. Hashmi; S. Khan; Y. -T. Peng; Y. Tsao; H. -M. Wang";2022;Deepfake technology has advanced a lot, but it is a double-sided sword for the community. One can use it for beneficial purposes, such as restoring vintage content in old movies, or for nefarious purposes, such as creating fake footage to manipulate the public and distribute non-consensual pornography. A lot of work has been done to combat its improper use by detecting fake footage with good performance thanks to the availability of numerous public datasets and unimodal deep learning-based models. However, these methods are insufficient to detect multimodal manipulations, such as both visual and acoustic. This work proposes a novel lip-reading-based multi-modal Deepfake detection method called Lip Sync Matters. It targets high-level semantic features to exploit the mismatch between the lip sequence extracted from the video and the synthetic lip sequence generated from the audio by the Wav2lip model to detect forged videos. Experimental results show that the proposed method outperforms several existing unimodal, ensemble, and multimodal methods on the publicly available multimodal FakeAVCeleb dataset.;;IEE;WAHR
911;Countering Deepfakes using an Improved Advanced CNN and its Ensemble with Pretrained Models;"A. Mathur; M. Tejpal; K. Bhargava; K. Natarajan; M. Singh";2024;The extensive spread of DeepFake images on the internet has emerged as a significant challenge, with applications ranging from harmless entertainment to harmful acts like blackmail, misinformation, and spreading false propaganda. To tackle this issue, this paper introduces a sophisticated DeepFake detection model designed to identify and mitigate the increase of these deceptive images. The model architecture integrates an ensemble approach, combining the strengths of two pre-trained Convolutional Neural Network (CNN) modelsMobileNet and Xceptionwith a novel CNN architecture, the Advanced CNN (ACNN). This rigorous validation process enabled the model to achieve a high accuracy rate of 97.89% in detecting DeepFakes. The successful implementation of this ensemble CNN approach demonstrates its effectiveness in distinguishing between real and fabricated imagery with high precision. This research makes a substantial contribution to the field of digital image forensics, offering a reliable tool for stakeholders across various sectors to identify and counteract the spread of DeepFake images online.;"DeepFake Detection;Convolutional Neural Networks (CNN);MobileNet;Xception;Advanced CNN (ACNN);Ensemble Learning;Generative Adversarial Networks (GANs)";IEE;WAHR
914;Detection of Photoplethysmography Manipulation in Video Forgery;"Y. Lin; E. Maiorana; B. Li; P. Campisi";2023;Recent studies have shown that physiological signals related to blood pressure and heart rate can be estimated in a contactless modality from facial videos using remote photoplethysmography (rPPG). This has paved the way to the development of techniques that can acquire and manipulate the rPPG signals recoverable from facial videos without affecting their visual appearance. The goal of this paper is to analyze the detectability of this new kind of forgery, here referred to as rPPG deepfake videos. Specifically, we propose a two-stream method based on the analysis of rPPG deepfake videos at both spatial and temporal levels. The experimental results obtained from tests performed on samples taken from two distinct databases demonstrate that our method performs better than popular deep learning methods in the rPPG deepfake detection task.;"Remote photoplethysmography (rPPG);Face forgery detection;Video forensics";IEE;WAHR
918;Using Grayscale Frequency Statistic to Detect Manipulated Faces in Wavelet-Domain;"G. -J. Wang; W. Li; Q. Jiang; X. Jin; X. -H. Cui";2021;Manipulating facial images results in negative influences on the social association, with deep generative models. Although many detection methods have been proposed, they have either designed sophisticated neural networks that lack enough interpretability, or found defects specific to one manipulation method. To address this issue, we propose a new approach to explore the defects of fake facial images after wavelet transform and call it GFS (Grayscale Frequency Statistics). First, we utilize Haar wavelet transformation to decompose the image into low-frequency approximation, horizontal detail, vertical detail, and diagonal detail. The GFS of real and fake images exhibit different distribution and forms in these four subbands. We qualitatively analyze these differences and quantify them as weights. Then, these four subband images are used to train four CNNs respectively, and the obtained detection results also verify the differences in GFS. After that, we combine the prediction results of the four CNNs and the corresponding weights to further improve the detection performance. We conduct extensive experiments on 11 datasets generated by various facial manipulation methods, and the superior results show the effectiveness of our proposed approach. Our findings indicate that the fake images generated by the current facial manipulation methods cannot simulate real images in wavelet-domain.;;IEE;WAHR
925;Detecting Forged Facial Videos Using Convolutional Neural Networks;"N. Sambhu; S. Canavan";2023;In this paper, we propose to detect forged videos, of faces, in online videos. To facilitate this detection, we propose to use smaller (fewer parameters to learn) convolutional neural networks (CNN), for a data-driven approach to forged video detection. To validate our approach, we investigate the Face-Forensics public dataset detailing both frame-based and video-based results. The proposed method is shown to outperform current state of the art. We also perform an ablation study, analyzing the impact of batch size, number of filters, and number of network layers on the accuracy of detecting forged videos.;"deepfake;convolutional neural network;videos;deep learning";IEE;WAHR
931;Deep Fakes Image Animation Using Generative Adversarial Networks;"A. K. Manjula; R. Thirukkumaran; K. H. Raj; A. Athappan; R. P. Reddy";2022;The idea of picture activity is for the most part moving the pictures at a specific speed so the unaided eye can't detect the distinction. We intend to do the investigation so that for certain adjustments to the current structure that is the deepfake that does the examination without earlier information on the movement target. To do this, We will be training a dataset on a bunch of pictures and recordings for objects of a similar class (e.g., face, body, road view). As of late, a few uses of neural organizations (CNNs) have been applied to the genuine human head. The informational index can be prepared on many pictures and recordings to make practical talking heads. You can energize the first picture of an individual into an objective individual posture (driving video) while safeguarding the individual's appearance and body. In the mean time, in any case, frameworks are being fostered that can recognize recordings and activities produced by Deep-Fakes. Since this is a significant security issue. We energized pictures to create talking heads and tried different things with picture age Using the Deep-Fakes age's contingent generative threatening organization, the outcomes were reasonable. Likewise executed Deep-Fake Detector XceptionNet (a Deep Learning Algorithm that Detects Face Swaps in Videos) with slight adjustments to arrive at 95° exactness when identifying Deep-Fake. At last, you can without much of a stretch idiot Deepfake identifiers by executing an as of late acquainted method with quit making Deep-Fakes. XceptionNet had the option to accomplish a precision of under 30 in recognizing the Deep-Fake age when maddened.;"DeepFakes;ImageAnimation;GANS;Generative Adversarial Networks;cGanz;Co-lab;video;MonkeyNET";IEE;WAHR
932;Combating Deepfakes: Multi-LSTM and Blockchain as Proof of Authenticity for Digital Media;"C. C. Ki Chan; V. Kumar; S. Delaney; M. Gochoo";2020;"Malicious use of deep learning algorithms has allowed the proliferation of high realism fake digital content such as text, images, and videos, to exist on the internet as readily available and accessible consumable content. False information provided through algorithmically modified footage, images, audios, and videos (known as deepfakes), coupled with the virality of social networks, may cause major social unrest. The emergence of misinformation from fabricated digital content suggests the necessity for anti-disinformation methods such as deepfake detection algorithms or immutable metadata in order to verify the validity of digital content. Permissioned blockchain, notably Hyperledger Fabric 2.0, coupled with LSTMs for audio/video/descriptive captioning is a step towards providing a feasible tool for combating deepfake media. Original content would require the original artist attestation of untampered data. The smart contract combines a varied multiple LSTM networks into a process that allows for the tracing and tracking of a digital content's historical provenance. The result is a theoretical framework that enables proof of authenticity (PoA) for digital media using a decentralized blockchain using multiple LSTMs as a deep encoder for creating unique discriminative features; which is then compressed and hashed into a transaction. Our work assumes we trust the video at the point of reception. Our contribution is a decentralized blockchain framework of deep discriminative digital media to combat deepfakes.";"artificial intelligence;blockchain;computer vision;deepfake;smart contracts";IEE;WAHR