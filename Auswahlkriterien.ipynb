{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Literature_All.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLiterature_All.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     24\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPiloting_all.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Specify the output Excel file\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(input_path, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8-sig\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n\u001b[1;32m     26\u001b[0m df_output \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(output_path)  \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Filter the input DataFrame to only include the rows with error IDs\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# df = df[df['ID'].isin(error_ids)]\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m#     print(f\"Max ID: {max_id}\")\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m#     df = df[df['ID'] > max_id]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[1;32m   1881\u001b[0m     f,\n\u001b[1;32m   1882\u001b[0m     mode,\n\u001b[1;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[1;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[1;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m   1889\u001b[0m )\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[1;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[1;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Literature_All.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import anthropic\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "# load the API key from the .env file\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"ANTHROPIC_API_KEY\")\n",
    "\n",
    "# initialize the client\n",
    "client = anthropic.Anthropic(api_key=api_key)\n",
    "\n",
    "\n",
    "# Enter the Error IDs here to rerun the API call for these and replace the existing rows in the output Excel file\n",
    "error_ids = []\n",
    "\n",
    "# Global variables\n",
    "counter = 0  # To keep track of processed rows\n",
    "\n",
    "# Load the input CSV file and the output Excel file\n",
    "input_path_piloting = 'Auswahlkriterien_Piloting_Literatur.csv'\n",
    "input_path = 'Auswahlkriterien_Durchführung_Literatur.csv'\n",
    "\n",
    "output_path_piloting = 'Auswahlkriterien_Piloting.xlsx'  # Specify the output Excel file\n",
    "output_path = 'Auswahlkriterien_Durchführung.xlsx'  # Specify the output Excel file\n",
    "\n",
    "\n",
    "df = pd.read_csv(input_path, sep=',', encoding='utf-8-sig')  \n",
    "df_output = pd.read_excel(output_path)  \n",
    "\n",
    "# Filter the input DataFrame to only include the rows with error IDs\n",
    "# df = df[df['ID'].isin(error_ids)]\n",
    "\n",
    "\n",
    "# Start the counter from the max ID in the output file to continue from where it left off\n",
    "# max_id = df_output['ID'].max()\n",
    "# if pd.isna(max_id):\n",
    "#     print(\"No valid IDs in the file.\")\n",
    "# else:\n",
    "#     print(f\"Max ID: {max_id}\")\n",
    "#     df = df[df['ID'] > max_id]\n",
    "\n",
    "\n",
    "def append_to_excel(row, output_path):\n",
    "    global counter\n",
    "    row_df = pd.DataFrame([row])\n",
    "\n",
    "    # Load existing workbook and sheet\n",
    "    workbook = load_workbook(output_path)\n",
    "    sheet = workbook.active\n",
    "\n",
    "    # Check if ID exists in the sheet\n",
    "    existing_ids = [cell.value for cell in sheet['A']]  # Assuming 'A' is the column where IDs are stored\n",
    "\n",
    "    # If ID exists, replace the row\n",
    "    if row['ID'] in existing_ids and row['ID'] in error_ids:\n",
    "        # Find the row with the matching ID and replace it\n",
    "        row_index = existing_ids.index(row['ID']) + 1  # Excel is 1-indexed\n",
    "        for col_num, value in enumerate(row_df.iloc[0], start=1):\n",
    "            sheet.cell(row=row_index, column=col_num, value=value)\n",
    "            row[\"Error\"] = \"Error\"\n",
    "        print(f\"Replaced ID: {row['ID']}\")\n",
    "    else:\n",
    "        # Otherwise, append the new row\n",
    "        startrow = sheet.max_row + 1\n",
    "        for col_num, value in enumerate(row_df.iloc[0], start=1):\n",
    "            sheet.cell(row=startrow, column=col_num, value=value)\n",
    "        print(f\"Appended ID: {row['ID']}\")\n",
    "\n",
    "    # Save the workbook\n",
    "    workbook.save(output_path)\n",
    "    counter += 1\n",
    "    print(f\"Counter: {counter}\")\n",
    "\n",
    "\n",
    "def check_kriterien(row):\n",
    "    global response\n",
    "    global counter\n",
    "    try:\n",
    "        title = row['Title']\n",
    "        abstract = row['Abstract']\n",
    "\n",
    "        systemprompt = \"You are a reviewer conducting a systematic literature review, evaluating studies based on their title and abstracts and inclusion and exclusion criteria for their relevance. The title of the systematic literature review is 'Detection of visual deepfakes using machine learning'\"\n",
    "        \n",
    "        prompt = f\"\"\"\n",
    "            1. Carefully read the title in <title> and abstract provided in <abstract>.\n",
    "            2. Evaluate the study for relevance using the inclusion and exclusion criteria from <inclusion_criteria> and <exclusion_criteria>:\n",
    "            - Follow this guided approach for each criterion:\n",
    "                1. Use the **unique ID** assigned to each criterion for traceability.\n",
    "                2. Identify if the criterion applies based on the title and abstract.\n",
    "                3. Determine if the criterion is **met** (`true` or `false`)\n",
    "                4. If an inclusion criterion is not met or an exclusion criterion is met, provide a brief explanation.\n",
    "            3. Make a **final decision** about the study's relevance:\n",
    "            - If **all inclusion criteria** are met and **no exclusion criteria** is met, mark the study as `\"relevant\": true`.\n",
    "            - Otherwise, mark it as `\"relevant\": false`.\n",
    "            4. Return the evaluation results as a transparent JSON object following the structure in <desired_output>. Note: Is just an example for the structure of the output!\n",
    "            \n",
    "            <title>\n",
    "            {title}\n",
    "            </title>\n",
    "\n",
    "            <abstract>\n",
    "            {abstract}\n",
    "            </abstract>\n",
    "            \n",
    "            <inclusion_criteria>\n",
    "            - **IC1**: The focus of the work is on deepfake detection in videos and photos (visual deepfakes).\n",
    "            - **IC2**: Deepfake detection is achieved using machine learning methods (e.g. Logistic Regression, Support Vector Machines (SVMs), Convolutional Neural Networks (CNNs), Generative Adversarial Networks (GANs).\n",
    "            </inclusion_criteria>\n",
    "\n",
    "            <exclusion_criteria>\n",
    "            - **EC1**: Not written in English.\n",
    "            - **EC2**: The focus of the work is not on the detection of visual deepfakes.\n",
    "            - **EC3**: Focus on preventive methods for detecting and preventing manipulations, such as visual encoding of original images, instead of directly detecting visual manipulations\n",
    "            - **EC4**: It is a secondary study (e.g., survey, systematic literature review, meta-analysis\n",
    "            </exclusion_criteria>\n",
    "            \n",
    "        <desired_output>\n",
    "            {{\n",
    "                \"criteria_evaluation\": {{\n",
    "                    \"inclusion_criteria\": {{\n",
    "                        \"IC1\": {{\n",
    "                            \"decision\": True,\n",
    "                        }},\n",
    "                        \"IC2\": {{\n",
    "                            \"decision\": False,\n",
    "                            \"explanation\": \"brief explanation here\"\n",
    "                        }}\n",
    "                    }},\n",
    "                    \"exclusion_criteria\": {{\n",
    "                        \"EC1\": {{\n",
    "                            \"decision\": False\n",
    "                        }},\n",
    "                        \"EC2\": {{\n",
    "                            \"decision\": False\n",
    "                        }},\n",
    "                        \"EC3\": {{\n",
    "                            \"decision\": True,\n",
    "                            \"explanation\": \"brief explanation here\"\n",
    "                        }},\n",
    "                        \"EC4\": {{\n",
    "                            \"decision\": False\n",
    "                        }}\n",
    "                    }}\n",
    "                }},\n",
    "                \"relevant\": False\n",
    "            }}\n",
    "            </desired_output>\n",
    "\n",
    "            Just return the json object according to this structure.\n",
    "            \"\"\"\n",
    "        # API-Aufruf\n",
    "        response = client.messages.create(\n",
    "            model=\"claude-3-5-sonnet-20241022\",\n",
    "            max_tokens=2048,\n",
    "            system=systemprompt,\n",
    "            temperature=0,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # get the content of the response\n",
    "        content = response.content[0].text\n",
    "\n",
    "        data = json.loads(content)\n",
    "        # extract the relevant information\n",
    "        inclusion_criteria = data[\"criteria_evaluation\"][\"inclusion_criteria\"]\n",
    "        exclusion_criteria = data[\"criteria_evaluation\"][\"exclusion_criteria\"]\n",
    "        relevant = data[\"relevant\"]\n",
    "\n",
    "        # add the relevant information to the row\n",
    "        row[\"IC1\"] = inclusion_criteria[\"IC1\"].get(\"decision\", None)\n",
    "        row[\"IC2\"] = inclusion_criteria[\"IC2\"].get(\"decision\", None)\n",
    "        row[\"EC1\"] = exclusion_criteria[\"EC1\"].get(\"decision\", None)\n",
    "        row[\"EC2\"] = exclusion_criteria[\"EC2\"].get(\"decision\", None)\n",
    "        row[\"EC3\"] = exclusion_criteria[\"EC3\"].get(\"decision\", None)\n",
    "        row[\"EC4\"] = exclusion_criteria[\"EC4\"].get(\"decision\", None)\n",
    "        row[\"Auto Decision\"] = relevant\n",
    "        \n",
    "        # add the explanation to the row\n",
    "        row[\"IC1_explanation\"] = inclusion_criteria[\"IC1\"].get(\"explanation\", \"\")\n",
    "        row[\"IC2_explanation\"] = inclusion_criteria[\"IC2\"].get(\"explanation\", \"\")\n",
    "        row[\"EC1_explanation\"] = exclusion_criteria[\"EC1\"].get(\"explanation\", \"\")\n",
    "        row[\"EC2_explanation\"] = exclusion_criteria[\"EC2\"].get(\"explanation\", \"\")\n",
    "        row[\"EC3_explanation\"] = exclusion_criteria[\"EC3\"].get(\"explanation\", \"\")\n",
    "        row[\"EC4_explanation\"] = exclusion_criteria[\"EC4\"].get(\"explanation\", \"\")\n",
    "\n",
    "        row[\"input_tokens\"] = response.usage.input_tokens\n",
    "        row[\"output_tokens\"] = response.usage.output_tokens\n",
    "\n",
    "    except Exception as e:\n",
    "        # falls ein Fehler auftritt, füge die ID zur error_ids-Liste hinzu\n",
    "        print(f\"Error bei ID: {row['ID']} : {e}\")\n",
    "        # add the ID to the error_ids list\n",
    "        error_ids.append(row['ID'])\n",
    "        row[\"IC1\"] = None\n",
    "        row[\"IC2\"] = None\n",
    "        row[\"EC1\"] = None\n",
    "        row[\"EC2\"] = None\n",
    "        row[\"EC3\"] = None\n",
    "        row[\"EC4\"] = None\n",
    "        row[\"Relevant\"] = None\n",
    "\n",
    "    append_to_excel(row, output_path)\n",
    "    return row\n",
    "\n",
    "# apply the function to the DataFrame\n",
    "df = df.apply(check_kriterien, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[379, 490]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "error_ids"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
